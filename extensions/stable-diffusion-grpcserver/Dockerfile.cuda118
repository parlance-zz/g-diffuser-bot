FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04 AS buildpytorch

# Basic updates. Do super early so we can cache for a long time
RUN apt update
RUN apt install -y curl
RUN apt install -y git

# Set up core python environment
RUN curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba

# Set up basic python
RUN /bin/micromamba -r /env -y create -c defaults -n sd-grpc-server python~=3.10.0 pip
# Install MKL from Intel - it's newer than conda-forge one
RUN /bin/micromamba -r /env -n sd-grpc-server install -c intel mkl=2022.2.0 mkl-devel=2022.2.0
# Install dependancies
RUN /bin/micromamba -r /env -n sd-grpc-server install -c defaults astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclasses

# Make sure cmake looks in our enviroment
ENV CMAKE_PREFIX_PATH=/env/envs/sd-grpc-server

# Download magma
RUN curl -OL http://icl.utk.edu/projectsfiles/magma/downloads/magma-2.6.2.tar.gz
RUN tar xvzf magma-2.6.2.tar.gz
RUN mkdir -p /magma-2.6.2/build

# Modify magma CMakeFile to allow arbitrary CUDA arches
WORKDIR /magma-2.6.2
COPY docker_support/CMakeFile.txt.diff /
RUN patch < /CMakeFile.txt.diff

 # Build it
WORKDIR /magma-2.6.2/build
RUN /bin/micromamba -r /env -n sd-grpc-server run cmake .. -DMAGMA_WITH_MKL=1 -DUSE_FORTRAN=off -DGPU_TARGET="All" -DCUDA_ARCH_LIST="-gencode arch=compute_60,code=sm_60 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89"
#RUN /bin/micromamba -r /env -n sd-grpc-server run cmake .. -DMAGMA_WITH_MKL=1 -DUSE_FORTRAN=off -DGPU_TARGET="sm_89" -DCUDA_ARCH_LIST="-gencode arch=compute_89,code=sm_89"
RUN /bin/micromamba -r /env -n sd-grpc-server run make lib
RUN /bin/micromamba -r /env -n sd-grpc-server run make sparse-lib
RUN /bin/micromamba -r /env -n sd-grpc-server run make install

# Move into env (TODO: just build here in the first place)
RUN mv /usr/local/magma/lib/*.so /env/envs/sd-grpc-server/lib/
RUN mv /usr/local/magma/lib/pkgconfig/*.pc /env/envs/sd-grpc-server/lib/pkgconfig/
RUN mv /usr/local/magma/include/* /env/envs/sd-grpc-server/include/

# Download pytorch
WORKDIR /
RUN git clone https://github.com/pytorch/pytorch

WORKDIR /pytorch
RUN git checkout -b v1.12.1 tags/v1.12.1
RUN git submodule update --init --recursive

# Built and install pytorch
ENV FORCE_CUDA=1
ENV TORCH_CUDA_ARCH_LIST="6.0;7.0;7.5;8.0;8.6;8.9"
ENV USE_KINETO=0
ENV USE_CUPTI=0
ENV PYTORCH_BUILD_VERSION=1.12.1+cu118 
ENV PYTORCH_BUILD_NUMBER=0 
COPY docker_support/cpp_extension.py /pytorch/torch/utils/
RUN /bin/micromamba -r /env -n sd-grpc-server run python tools/generate_torch_version.py --is_debug false --cuda_version 11.8
RUN /bin/micromamba -r /env -n sd-grpc-server run python setup.py install

# Download torchvision
WORKDIR /
RUN git clone https://github.com/pytorch/vision

WORKDIR /vision
RUN git checkout -b v0.13.1 tags/v0.13.1

# Install torchvision dependancies
RUN /bin/micromamba -r /env -n sd-grpc-server install -c defaults libpng jpeg

# Built and install torchvision
ENV BUILD_VERSION=0.13.1+cu118
RUN /bin/micromamba -r /env -n sd-grpc-server run python setup.py install






FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu20.04 AS devbase

# Basic updates. Do super early so we can cache for a long time
RUN apt update
RUN apt install -y curl
RUN apt install -y git

# Copy core python environment from pytorchbuild
RUN mkdir -p /env
COPY --from=buildpytorch /bin/micromamba /bin
COPY --from=buildpytorch /env /env/




FROM devbase AS regularbase

# Install dependancies
ENV FLIT_ROOT_INSTALL=1
RUN /bin/micromamba -r /env -n sd-grpc-server install -c defaults flit

# We copy only the minimum for flit to run so avoid cache invalidation on code changes
COPY pyproject.toml .
COPY sdgrpcserver/__init__.py sdgrpcserver/
RUN touch README.md
RUN /bin/micromamba -r /env -n sd-grpc-server run flit install --pth-file
RUN /bin/micromamba -r /env -n sd-grpc-server run pip cache purge

# Setup NVM & Node for Localtunnel
ENV NVM_DIR=/nvm
ENV NODE_VERSION=16.18.0

RUN mkdir -p $NVM_DIR

RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash \
    && . $NVM_DIR/nvm.sh \
    && nvm install $NODE_VERSION \
    && nvm alias default $NODE_VERSION \
    && nvm use default




# Build Xformers

FROM devbase AS xformersbase

RUN git clone https://github.com/facebookresearch/xformers.git
WORKDIR /xformers
RUN git submodule update --init --recursive
RUN /bin/micromamba -r /env -n sd-grpc-server run pip install -r requirements.txt

ENV FORCE_CUDA=1
ENV TORCH_CUDA_ARCH_LIST="6.0;7.0;7.5;8.0;8.6;8.9"

RUN /bin/micromamba -r /env -n sd-grpc-server run pip install .

RUN tar cvjf /xformers.tbz /env/envs/sd-grpc-server/lib/python3.*/site-packages/xformers*




FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04 AS main

COPY --from=regularbase /bin/micromamba /bin/
RUN mkdir -p /env/envs
COPY --from=regularbase /env/envs /env/envs/
RUN mkdir -p /nvm
COPY --from=regularbase /nvm /nvm/

# Setup NVM & Node for Localtunnel
ENV NVM_DIR=/nvm
ENV NODE_VERSION=16.18.0

ENV NODE_PATH $NVM_DIR/versions/node/v$NODE_VERSION/lib/node_modules
ENV PATH      $NVM_DIR/versions/node/v$NODE_VERSION/bin:$PATH

RUN npm install -g localtunnel

# Now we can copy everything we need
COPY sdgrpcserver /sdgrpcserver/
COPY server.py .

# Set up some config files
RUN mkdir -p /huggingface
RUN mkdir -p /weights
RUN mkdir -p /config
COPY engines.yaml /config/

# Set up some environment files

ENV HF_HOME=/huggingface
ENV HF_API_TOKEN=mustset
ENV SD_ENGINECFG=/config/engines.yaml
ENV SD_WEIGHT_ROOT=/weights

CMD [ "/bin/micromamba", "-r", "env", "-n", "sd-grpc-server", "run", "python", "./server.py" ]




FROM main as xformers

COPY --from=xformersbase /xformers/requirements.txt /
RUN /bin/micromamba -r /env -n sd-grpc-server run pip install -r requirements.txt
RUN rm requirements.txt
COPY --from=xformersbase /xformers.tbz /
RUN tar xvjf /xformers.tbz
RUN rm /xformers.tbz

CMD [ "/bin/micromamba", "-r", "env", "-n", "sd-grpc-server", "run", "python", "./server.py" ]


