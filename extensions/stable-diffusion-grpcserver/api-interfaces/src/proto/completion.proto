syntax = 'proto3';
package gooseai;
option go_package = "./;completion";

message Token {
  string text = 1;
  uint32 id = 2;
}

message Tokens {
  repeated Token tokens = 1;
}

// The input can be provided in either string or sequence of tokens. Extensible
// to other input types in the future, thanks to Protobuf.
message Prompt {
  oneof prompt {
    string text = 1;
    Tokens tokens = 2;
  }
}

// Pairing of token sequences with bias values.
message LogitBias {
  Tokens tokens = 1;
  double bias = 2;
}

message LogitBiases {
  repeated LogitBias biases = 1;
}

// Reasons why the request finished
enum FinishReason {
  NULL = 0;
  LENGTH = 1;
  STOP = 2;
  ERROR = 3;
}

message FrequencyParams {
  optional double presence_penalty = 1;         // OAI: -2.0 to 2.0, penalize new tokens if seen in text
  optional double frequency_penalty = 2;        // OAI: -2.0 to 2.0, penalize new tokens based on fqcy
  optional double repetition_penalty = 3;       // GAI: Factor to divide logits
  optional double repetition_penalty_slope = 4; // GAI: slope applied against rp
  optional uint32 repetition_penalty_range = 5; // GAI: number of tokens for rp
}

enum SamplingMethod {
  NONE = 0;
  TEMPERATURE = 1;
  TOP_K = 2;
  TOP_P = 3;
  TFS = 4;
  TOP_A = 5;
  TYPICAL_P = 6;
}

message SamplingParams {
  repeated SamplingMethod order = 1;      // GAI: Order to apply transforms to
  optional double temperature = 2;        // OAI: What sampling temperature to
  optional double top_p = 3;              // OAI: nucleus sampling value
  optional uint32 top_k = 4;              // GAI: truncates logits to highest N
  optional double tail_free_sampling = 5; // GAI: tail free sampling
  optional double typical_p = 6;          // GAI: typical sampling
  optional double top_a = 7;              // GAI: Kurumuz's wild invention
}

// Model parameters
message ModelParams {
  optional SamplingParams sampling_params = 1;
  optional FrequencyParams frequency_params = 2; // Rep frequency parameters
  optional LogitBiases logit_bias = 3;           // OAI: Modify likelihood of specific tokens appearing.
}

message Echo {
  optional int32 index = 1; // GAI: Prompt token index to begin echo at, absence or zero is 'entire prompt'
}

enum NumType {
  FP16 = 0;
  FP32 = 1;
  BF16 = 2;
}

message ModuleEmbedding {
  string id = 1;                    // GAI: id of embedding, straight key or filename get
  string key = 2;                   // GAI: key to decrypt embedding
}

message Tensor {
  NumType typ = 1;                  // GAI: Numeric type of tensor.
  repeated uint32 dims = 2;         // GAI: Tensor dimensions.
  bytes data = 3;                   // GAI: Linear bytes serialization of tensor.
}

message Embedding {
  oneof embedding {
    Tensor raw = 1;                 // GAI: Serialization of an embedding tensor.
    ModuleEmbedding module = 2;     // GAI: ID + key of embedding to retrieve from object store
  }
  optional uint32 pos = 3;          // GAI: Where in the context to insert the embedding.
}

message EngineParams {
  optional uint32 max_tokens = 1;   // OAI: The maximum number of tokens to generate
  optional uint32 completions = 2;  // OAI: How many completions to generate per prompt
  optional uint32 logprobs = 3;     // OAI: Return N logprobs on most likely and picked tokens
  optional Echo echo = 4;           // OAI: Echo back the prompt in addition to completion
  optional uint32 best_of = 5;      // OAI: Generate # of `best_of` w. lowest logprob
  repeated Prompt stop = 6;         // OAI: Up to 4 sequences where API will stop generating
  optional uint32 min_tokens = 7;   // GAI: Minimum tokens before stop sequences activate.
}

message RequestMeta {
  optional bool streaming = 1;
}

// A 'generation' request.
message Request {
  string engine_id = 1;                    // OAI: The ID of the engine to use for this request
  repeated Prompt prompt = 2;              // OAI: The prompt(s) to generate completions for
  optional ModelParams model_params = 3;   //
  optional EngineParams engine_params = 4; //
  optional string request_id = 5;          // GAI: Request UUID, will be assigned if blank
  repeated Embedding embeddings = 6;       // GAI: Embeddings for model. 
  optional uint64 origin_received = 7;     // GAI: When we received the request from the origin.
  optional RequestMeta meta = 8;           // GAI: Optional metadata about our request.
}

message LogProb {
  Token token = 1;                       // tuple of token ...
  optional double logprob = 2;           // ... with logprobs
  optional double logprob_before = 3;    // ... and optionally, logprobs before processing
}

message TokenLogProbs {
  repeated LogProb logprobs = 1;        // zero or more logprobs
}

message LogProbs {
  TokenLogProbs tokens = 1;              // OAI: logprobs of chosen tokens
  repeated uint32 text_offset = 2;       // OAI: indexed offsets of tokens
  repeated TokenLogProbs top = 3;        // OAI: top N logprobs after processing
  repeated TokenLogProbs top_before = 4; // GAI: top N logprobs before processing
}

message Completion {
  string text = 1;                  // OAI: The completion in string format.
  uint32 index = 2;                 // OAI: The index of the completion.
  LogProbs logprobs = 3;            // OAI: Our answer with logprobs
  FinishReason finish_reason = 4;   // OAI: Why the completions finished.
  uint32 token_index = 5;           // GAI: where the completion starts
  uint64 started = 6;               // GAI: when the completion was started
}

message AnswerMeta {
  optional string gpu_id = 1;
  optional string cpu_id = 2;
  optional string node_id = 3;
}

message Answer {
  string answer_id = 1;             // OAI: Unique answer ID.
  uint64 created = 2;               // OAI: When the answer was created at.
  string model = 3;                 // OAI: What model and version answered.
  repeated Completion choices = 4;
  optional string request_id = 5;   // GAI: Request UUID this answer is to.
  uint64 inference_received = 6;    // GAI: When the request was received
  optional AnswerMeta meta = 7;     // GAI: Return request servicing metadata
}

service CompletionService {
  rpc Completion (Request) returns (stream Answer) {};
}